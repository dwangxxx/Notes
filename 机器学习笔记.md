## 决策树构造
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。
(1) 开始：构建根节点，将所有数据都放在根节点，选择一个最优特征，按照这一特征将训练是数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
(2) 如果这些子集已经能够被基本正确分类，那么构建叶子节点，并将这些子集分到所对应的叶节点去。
(3) 如果还有子集不能被正确分类，那么对这些子集选择新的最优特征，继续对其进行分那个，构建相应的节点，如此递归进行，直至所有训练数据子集被基本分类正确，或者没有合适的特征为止。
(4) 每个子集都被分到叶子结点上，即都有了明确的类，这样就生成了一颗决策树。

## AdaBoost
Boosting是一种可将弱学习器提升为强学习器的算法，它的3个体学习器之间存在强依赖关系、且必须串行生成，是集成学习的代表算法之一。通过构建并结合多个学习器来完成任务，有时候也被称为多分类器系统。
Boost算法涉及到两个部分，加法模型和前向分布算法。加法模型就是说强分类器是由一系列弱分类器线性相加而成。前向分布算法就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础训练得来的。
(1) 确定一个分类模型
(2) 重点观察出现分类错误的数据，产生新的模型，与原模型组合以改进模型
(3) 重复上述过程
